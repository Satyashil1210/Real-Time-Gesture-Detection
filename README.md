ğŸš€ Real-Time Gesture Detection
âœ‹ Static + ğŸ”„ Dynamic Gesture Recognition Roadmap

A production-structured real-time hand gesture recognition system built using MediaPipe, OpenCV, and modular ML-ready architecture.

Currently, the system supports fast static gesture recognition using a landmark-based heuristic engine.
The architecture is designed to evolve into a deep learning-based dynamic gesture recognition system.

âœ¨ Current Capabilities (v1 â€“ Static Gesture Engine)

âœ… Real-time hand detection using MediaPipe
âœ… Landmark-based rule engine for gesture classification
âœ… Avatar + label rendering system
âœ… CSV-based gesture registry (datasets/gestures.csv)
âœ… Modular & scalable code structure
âœ… Webcam live demo ready
âœ… Future-ready ML pipeline integration

ğŸ”® Vision (v2 â€“ ML + Dynamic Gesture Recognition)

Upcoming upgrades include:

ğŸ”„ Temporal gesture recognition (dynamic gestures)
ğŸ§  LSTM / GRU / MLP based classifier
ğŸ“Š Sliding window landmark buffering
ğŸ“¦ ONNX / TensorFlow Lite export
ğŸŒ Streamlit interactive web UI
ğŸ“ Automatic dataset builder
âš¡ Real-time FPS optimization
ğŸ“± Mobile / embedded deployment support

ğŸ— System Architecture
Camera Input ğŸ¥
        â†“
MediaPipe Hand Landmarks âœ‹
        â†“
Feature Extraction ğŸ“Š
        â†“
Static Rule Engine (Current Version)
        â†“
ML Classifier (Upcoming Version)
        â†“
Gesture ID
        â†“
CSV Mapping
        â†“
Avatar + Label Rendering ğŸ–¼

ğŸ“‚ Project Structure
Real-Time-Gesture-Detection/
â”œâ”€â”€ assets/avatars        # Gesture avatars (png / jpg)
â”œâ”€â”€ datasets              # CSV gesture definitions
â”œâ”€â”€ data/raw              # Captured frames
â”œâ”€â”€ data/processed        # Landmark feature files
â”œâ”€â”€ models/checkpoints    # Trained models (future)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ inference         # Real-time inference pipeline
â”‚   â”œâ”€â”€ detection         # MediaPipe abstraction
â”‚   â”œâ”€â”€ capture           # Dataset recording tools
â”‚   â”œâ”€â”€ processing        # Buffers & preprocessing
â”‚   â”œâ”€â”€ training          # ML training modules
â”‚   â””â”€â”€ app               # Streamlit UI layer

â–¶ï¸ How to Run (Live Demo)
1ï¸âƒ£ Activate Virtual Environment
.\venv\Scripts\Activate.ps1

2ï¸âƒ£ Run Static Real-Time Demo
python src/inference/live_gesture_demo.py


Press q to exit.

3ï¸âƒ£ Run Web UI (Streamlit)
streamlit run src/app/web_app_placeholder.py


Open in browser:

http://localhost:8501

ğŸ—‚ Dataset & Gesture Registry

Gestures are centrally defined in:

datasets/gestures.csv


Example format:

id,label,meaning,avatar
0,neutral,Neutral,neutral.png
1,victory,Victory Sign,victory.jpg
2,ok,OK Gesture,ok.jpg


This ensures prediction logic is decoupled from UI rendering.

ğŸ›  Tech Stack

ğŸ Python 3.10
ğŸ‘ MediaPipe
ğŸ“¸ OpenCV
ğŸ“Š NumPy
âš™ TensorFlow Lite (ML-ready)
ğŸŒ Streamlit
